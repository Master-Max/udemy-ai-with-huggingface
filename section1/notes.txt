Transformers

Encoder (left block)
    converts words into tokens (vectors)
    adds info
    gets data from the user
    add attention - each word pays attention to the other words in the sentence

Decoder (right block)
    generates output tokens
    only looks at previous words

________

Tokens add data in the Encoder phase
[CLS] - start of sentence
[SEP] = end of sentence

________

Uses google colab

> from transformers import AutoTokenizer

CLS and SEP change depending on model being used

________

Some models need only encoder or only Decoder

Encoder Only:
    - Text Classifiication (identifying spam or not spam)
Named Entity Recognition (NER):
    - identifying John as a person and London as a location
Sentiment Analysis:
    - Is a review of a movie positive or negative
Text Similarity
    - Compare 2 sentences to see how similar they AutoTokenizer


Example Full Models (That I can probably find on Hugging Face)
    - ALBERT
    - BERT
    - DistilBERT
    - ELECTRA
    - RoBERTa

________

Decoder Only:

(does actually need an encoder to make a token) ???

Text Generation
Text Summarization
Speech Recognition

Full Models
    - CTRL
    - GPT, GPT 1,2,3,4,4o
    - Claude, Gemini


________

Multi Level Attention and Feed Forward

________

How Transformers work

Encoder - text to representation
Decoder - representation to output

Tokenization - Encoder takes text and tokenizes it into layers
    - embeded into vectors using embedding layers

Embedding   EMBEDDING!
    - way to represent words, sentences or other types of data as Numberical Vectors
    - Vectors capture the meaning, relationships and sturcture of the data
    - Very important
    - preserves meaning with multi dimentional vectors
    - also uses positional context ( I think position in sentence is a layer that the token will have a vector on )

Multi-Head Attention (or Self Attention)
    - Each token embedding is projected into three vectors:
        * Query (Q) what is token looking for?
        * Key (K) features of token that other tokens can reference
        * Value (V) imformation that the token contributes

    The more meaningful the embeddings, the better the attention mechanism

Multi-Head Attention works in parallel
    Each head focuses on different relationships in the sentence
    one might forus on grammer another might focus on the meaning

FFNN - Feed Forward Neural Network
    Data is normalized and sent to FFNN, each token is independently processed
    Adds abstractions?????

Decoder - takes input from encoder and perform action of predicting the next work in the sequence

All of this is based on the Attention is All You Need paper

________

NLP momentum with Transformers

NLP - subfield of AI that focuses on enabling machines to understand and interpret human language in a meaninful way

Earlier NLP Models
    1. Recurrent Neural Networks (RNNs)
    2. Word2Vec
    3. Long-short Term memory (LSTM)
    4. n-gram

Pre Transformers

NLPs struggled with long range dependencies and context. Lost context of subject in long sentences.

Transformers are very good at text translaion. Used in wide range of NLP tasks

LLMs

GPT-4 and Claude are gneral purpose language models that can be applied to various NLP taks with minimal fine-tuning

________

End of section 1

Quiz Qs

1. Supervised learning (Leaning from labeled data to make predictions or classifications)

2. Multi-head attention (Allows the model to learn different representations or contexts from the same input in parallel)

3. NPL task that can be handled by Transformer-based models (Machine translation)



